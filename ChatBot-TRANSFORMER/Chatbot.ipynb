{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('movie_lines.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
    "conversations = open('movie_conversations.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary that maps each line and its id\n",
    "id2line = {}\n",
    "for line in lines:\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    if len(_line) == 5:\n",
    "        id2line[_line[0]] = _line[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of all of the conversations\n",
    "conversations_ids = []\n",
    "for conversation in conversations[:-1]:\n",
    "    _conversation = conversation.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n",
    "    conversations_ids.append(_conversation.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting separately the questions and the answers\n",
    "questions = []\n",
    "answers = []\n",
    "for conversation in conversations_ids:\n",
    "    for i in range(len(conversation) - 1):\n",
    "        questions.append(id2line[conversation[i]])\n",
    "        answers.append(id2line[conversation[i+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing a first cleaning of the texts\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}+=~|.?,]\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the questions\n",
    "clean_questions = []\n",
    "for question in questions:\n",
    "    clean_questions.append(clean_text(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the answers\n",
    "clean_answers = []\n",
    "for answer in answers:\n",
    "    clean_answers.append(clean_text(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['can we make this quick  roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad  again',\n",
       " 'well i thought we would start with pronunciation if that is okay with you',\n",
       " 'not the hacking and gagging and spitting part  please',\n",
       " 'you are asking me out  that is so cute what is your name again',\n",
       " \"no no it's my fault  we didn't have a proper introduction \",\n",
       " 'cameron',\n",
       " 'the thing is cameron  i am at the mercy of a particularly hideous breed of loser  my sister  i cannot date until she does',\n",
       " 'why',\n",
       " 'unsolved mystery  she used to be really popular when she started high school then it was just like she got sick of it or something',\n",
       " 'gosh if only we could find kat a boyfriend']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_questions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['well i thought we would start with pronunciation if that is okay with you',\n",
       " 'not the hacking and gagging and spitting part  please',\n",
       " \"okay then how 'bout we try out some french cuisine  saturday  night\",\n",
       " 'forget it',\n",
       " 'cameron',\n",
       " 'the thing is cameron  i am at the mercy of a particularly hideous breed of loser  my sister  i cannot date until she does',\n",
       " 'seems like she could get a date easy enough',\n",
       " 'unsolved mystery  she used to be really popular when she started high school then it was just like she got sick of it or something',\n",
       " 'that is a shame',\n",
       " 'let me see what i can do']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_answers[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing text\n",
    "tokenizer_qst = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    clean_questions, target_vocab_size=2**13)\n",
    "tokenizer_ans = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    clean_answers, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE_QST = tokenizer_qst.vocab_size + 2 \n",
    "VOCAB_SIZE_ANS = tokenizer_ans.vocab_size + 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [[VOCAB_SIZE_QST-2] + tokenizer_qst.encode(sentence) + [VOCAB_SIZE_QST-1]\n",
    "          for sentence in clean_questions]\n",
    "outputs = [[VOCAB_SIZE_ANS-2] + tokenizer_ans.encode(sentence) + [VOCAB_SIZE_ANS-1]\n",
    "           for sentence in clean_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized string is [48, 303, 5, 34, 123, 6672, 78, 48, 716, 619, 754, 7901]\n",
      "The original string: she used to be really popular when she started high school.\n"
     ]
    }
   ],
   "source": [
    "sample_string = 'she used to be really popular when she started high school.'\n",
    "\n",
    "tokenized_string = tokenizer_qst.encode(sample_string)\n",
    "print ('Tokenized string is {}'.format(tokenized_string))\n",
    "original_string = tokenizer_qst.decode(tokenized_string)\n",
    "print ('The original string: {}'.format(original_string))\n",
    "assert original_string == sample_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 ----> she \n",
      "303 ----> used \n",
      "5 ----> to \n",
      "34 ----> be \n",
      "123 ----> really \n",
      "6672 ----> popular \n",
      "78 ----> when \n",
      "48 ----> she \n",
      "716 ----> started \n",
      "619 ----> high \n",
      "754 ----> school\n",
      "7901 ----> .\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer_qst.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove too long sentences\n",
    "MAX_LENGTH = 20\n",
    "idx_to_remove = [count for count, sent in enumerate(inputs)\n",
    "                 if len(sent) > MAX_LENGTH]\n",
    "for idx in reversed(idx_to_remove):\n",
    "    del inputs[idx]\n",
    "    del outputs[idx]\n",
    "idx_to_remove = [count for count, sent in enumerate(outputs)\n",
    "                 if len(sent) > MAX_LENGTH]\n",
    "for idx in reversed(idx_to_remove):\n",
    "    del inputs[idx]\n",
    "    del outputs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs/outputs\n",
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                       value=0,\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=MAX_LENGTH)\n",
    "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
    "                                                        value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "### Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "class PositionalEncoding(layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "    \n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n",
    "        return pos * angles\n",
    "\n",
    "    def call(self, inputs):\n",
    "        seq_length = inputs.shape.as_list()[-2]\n",
    "        d_model = inputs.shape.as_list()[-1]\n",
    "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
    "                                 np.arange(d_model)[np.newaxis, :],\n",
    "                                 d_model)\n",
    "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "        pos_encoding = angles[np.newaxis, ...]\n",
    "        return inputs + tf.cast(pos_encoding, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(queries, keys, values, mask):\n",
    "    product = tf.matmul(queries, keys, transpose_b=True)\n",
    "    \n",
    "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
    "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scaled_product += (mask * -1e9)\n",
    "    \n",
    "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
    "    \n",
    "    return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head attention sublayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "    \n",
    "    def __init__(self, nb_proj):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.nb_proj = nb_proj\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        assert self.d_model % self.nb_proj == 0\n",
    "        \n",
    "        self.d_proj = self.d_model // self.nb_proj\n",
    "        \n",
    "        self.query_lin = layers.Dense(units=self.d_model)\n",
    "        self.key_lin = layers.Dense(units=self.d_model)\n",
    "        self.value_lin = layers.Dense(units=self.d_model)\n",
    "        \n",
    "        self.final_lin = layers.Dense(units=self.d_model)\n",
    "        \n",
    "    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n",
    "        shape = (batch_size,\n",
    "                 -1,\n",
    "                 self.nb_proj,\n",
    "                 self.d_proj)\n",
    "        splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\n",
    "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\n",
    "    \n",
    "    def call(self, queries, keys, values, mask):\n",
    "        batch_size = tf.shape(queries)[0]\n",
    "        \n",
    "        queries = self.query_lin(queries)\n",
    "        keys = self.key_lin(keys)\n",
    "        values = self.value_lin(values)\n",
    "        \n",
    "        queries = self.split_proj(queries, batch_size)\n",
    "        keys = self.split_proj(keys, batch_size)\n",
    "        values = self.split_proj(values, batch_size)\n",
    "        \n",
    "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
    "        \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        concat_attention = tf.reshape(attention,\n",
    "                                      shape=(batch_size, -1, self.d_model))\n",
    "        \n",
    "        outputs = self.final_lin(concat_attention)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.FFN_units = FFN_units\n",
    "        self.nb_proj = nb_proj\n",
    "        self.dropout_rate = dropout_rate\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        \n",
    "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
    "        self.dense_2 = layers.Dense(units=self.d_model)\n",
    "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, inputs, mask, training):\n",
    "        attention = self.multi_head_attention(inputs,\n",
    "                                              inputs,\n",
    "                                              inputs,\n",
    "                                              mask)\n",
    "        attention = self.dropout_1(attention, training=training)\n",
    "        attention = self.norm_1(attention + inputs)\n",
    "        \n",
    "        outputs = self.dense_1(attention)\n",
    "        outputs = self.dense_2(outputs)\n",
    "        outputs = self.dropout_2(outputs, training=training)\n",
    "        outputs = self.norm_2(outputs + attention)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 nb_layers,\n",
    "                 FFN_units,\n",
    "                 nb_proj,\n",
    "                 dropout_rate,\n",
    "                 vocab_size,\n",
    "                 d_model,\n",
    "                 name=\"encoder\"):\n",
    "        super(Encoder, self).__init__(name=name)\n",
    "        self.nb_layers = nb_layers\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding()\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        self.enc_layers = [EncoderLayer(FFN_units,\n",
    "                                        nb_proj,\n",
    "                                        dropout_rate) \n",
    "                           for _ in range(nb_layers)]\n",
    "    \n",
    "    def call(self, inputs, mask, training):\n",
    "        outputs = self.embedding(inputs)\n",
    "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        outputs = self.pos_encoding(outputs)\n",
    "        outputs = self.dropout(outputs, training)\n",
    "        \n",
    "        for i in range(self.nb_layers):\n",
    "            outputs = self.enc_layers[i](outputs, mask, training)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.FFN_units = FFN_units\n",
    "        self.nb_proj = nb_proj\n",
    "        self.dropout_rate = dropout_rate\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        \n",
    "        # Self multi head attention\n",
    "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Multi head attention combined with encoder output\n",
    "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Feed foward\n",
    "        self.dense_1 = layers.Dense(units=self.FFN_units,\n",
    "                                    activation=\"relu\")\n",
    "        self.dense_2 = layers.Dense(units=self.d_model)\n",
    "        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
    "        attention = self.multi_head_attention_1(inputs,\n",
    "                                                inputs,\n",
    "                                                inputs,\n",
    "                                                mask_1)\n",
    "        attention = self.dropout_1(attention, training)\n",
    "        attention = self.norm_1(attention + inputs)\n",
    "        \n",
    "        attention_2 = self.multi_head_attention_2(attention,\n",
    "                                                  enc_outputs,\n",
    "                                                  enc_outputs,\n",
    "                                                  mask_2)\n",
    "        attention_2 = self.dropout_2(attention_2, training)\n",
    "        attention_2 = self.norm_2(attention_2 + attention)\n",
    "        \n",
    "        outputs = self.dense_1(attention_2)\n",
    "        outputs = self.dense_2(outputs)\n",
    "        outputs = self.dropout_3(outputs, training)\n",
    "        outputs = self.norm_3(outputs + attention_2)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 nb_layers,\n",
    "                 FFN_units,\n",
    "                 nb_proj,\n",
    "                 dropout_rate,\n",
    "                 vocab_size,\n",
    "                 d_model,\n",
    "                 name=\"decoder\"):\n",
    "        super(Decoder, self).__init__(name=name)\n",
    "        self.d_model = d_model\n",
    "        self.nb_layers = nb_layers\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding()\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        \n",
    "        self.dec_layers = [DecoderLayer(FFN_units,\n",
    "                                        nb_proj,\n",
    "                                        dropout_rate) \n",
    "                           for i in range(nb_layers)]\n",
    "    \n",
    "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
    "        outputs = self.embedding(inputs)\n",
    "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        outputs = self.pos_encoding(outputs)\n",
    "        outputs = self.dropout(outputs, training)\n",
    "        \n",
    "        for i in range(self.nb_layers):\n",
    "            outputs = self.dec_layers[i](outputs,\n",
    "                                         enc_outputs,\n",
    "                                         mask_1,\n",
    "                                         mask_2,\n",
    "                                         training)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocab_size_enc,\n",
    "                 vocab_size_dec,\n",
    "                 d_model,\n",
    "                 nb_layers,\n",
    "                 FFN_units,\n",
    "                 nb_proj,\n",
    "                 dropout_rate,\n",
    "                 name=\"transformer\"):\n",
    "        super(Transformer, self).__init__(name=name)\n",
    "        \n",
    "        self.encoder = Encoder(nb_layers,\n",
    "                               FFN_units,\n",
    "                               nb_proj,\n",
    "                               dropout_rate,\n",
    "                               vocab_size_enc,\n",
    "                               d_model)\n",
    "        self.decoder = Decoder(nb_layers,\n",
    "                               FFN_units,\n",
    "                               nb_proj,\n",
    "                               dropout_rate,\n",
    "                               vocab_size_dec,\n",
    "                               d_model)\n",
    "        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"lin_ouput\")\n",
    "    \n",
    "    def create_padding_mask(self, seq):\n",
    "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "    def create_look_ahead_mask(self, seq):\n",
    "        seq_len = tf.shape(seq)[1]\n",
    "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        return look_ahead_mask\n",
    "    \n",
    "    def call(self, enc_inputs, dec_inputs, training):\n",
    "        enc_mask = self.create_padding_mask(enc_inputs)\n",
    "        dec_mask_1 = tf.maximum(\n",
    "            self.create_padding_mask(dec_inputs),\n",
    "            self.create_look_ahead_mask(dec_inputs)\n",
    "        )\n",
    "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
    "        \n",
    "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
    "        dec_outputs = self.decoder(dec_inputs,\n",
    "                                   enc_outputs,\n",
    "                                   dec_mask_1,\n",
    "                                   dec_mask_2,\n",
    "                                   training)\n",
    "        \n",
    "        outputs = self.last_linear(dec_outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Hyper-parameters\n",
    "D_MODEL = 128 # 512\n",
    "NB_LAYERS = 4 # 6\n",
    "FFN_UNITS = 512 # 2048\n",
    "NB_PROJ = 8 # 8\n",
    "DROPOUT_RATE = 0.1 # 0.1\n",
    "\n",
    "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_QST,\n",
    "                          vocab_size_dec=VOCAB_SIZE_ANS,\n",
    "                          d_model=D_MODEL,\n",
    "                          nb_layers=NB_LAYERS,\n",
    "                          FFN_units=FFN_UNITS,\n",
    "                          nb_proj=NB_PROJ,\n",
    "                          dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                            reduction=\"none\")\n",
    "\n",
    "def loss_function(target, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
    "    loss_ = loss_object(target, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "        \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "leaning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(leaning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"ckpt/\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(\"Latest checkpoint restored!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Batch 0 Loss 3.2165 Accuracy 0.0000\n",
      "Epoch 1 Batch 50 Loss 3.8895 Accuracy 0.0118\n",
      "Epoch 1 Batch 100 Loss 3.8377 Accuracy 0.0320\n",
      "Epoch 1 Batch 150 Loss 3.8086 Accuracy 0.0389\n",
      "Epoch 1 Batch 200 Loss 3.7606 Accuracy 0.0423\n",
      "Epoch 1 Batch 250 Loss 3.6878 Accuracy 0.0443\n",
      "Epoch 1 Batch 300 Loss 3.6177 Accuracy 0.0457\n",
      "Epoch 1 Batch 350 Loss 3.5276 Accuracy 0.0467\n",
      "Epoch 1 Batch 400 Loss 3.4452 Accuracy 0.0474\n",
      "Epoch 1 Batch 450 Loss 3.3655 Accuracy 0.0480\n",
      "Epoch 1 Batch 500 Loss 3.3015 Accuracy 0.0490\n",
      "Epoch 1 Batch 550 Loss 3.2460 Accuracy 0.0505\n",
      "Epoch 1 Batch 600 Loss 3.1905 Accuracy 0.0525\n",
      "Epoch 1 Batch 650 Loss 3.1419 Accuracy 0.0548\n",
      "Epoch 1 Batch 700 Loss 3.0934 Accuracy 0.0571\n",
      "Epoch 1 Batch 750 Loss 3.0472 Accuracy 0.0595\n",
      "Epoch 1 Batch 800 Loss 3.0049 Accuracy 0.0617\n",
      "Epoch 1 Batch 850 Loss 2.9638 Accuracy 0.0637\n",
      "Epoch 1 Batch 900 Loss 2.9272 Accuracy 0.0655\n",
      "Epoch 1 Batch 950 Loss 2.8922 Accuracy 0.0673\n",
      "Epoch 1 Batch 1000 Loss 2.8614 Accuracy 0.0689\n",
      "Epoch 1 Batch 1050 Loss 2.8331 Accuracy 0.0704\n",
      "Epoch 1 Batch 1100 Loss 2.8063 Accuracy 0.0718\n",
      "Epoch 1 Batch 1150 Loss 2.7810 Accuracy 0.0731\n",
      "Epoch 1 Batch 1200 Loss 2.7557 Accuracy 0.0743\n",
      "Epoch 1 Batch 1250 Loss 2.7353 Accuracy 0.0756\n",
      "Epoch 1 Batch 1300 Loss 2.7151 Accuracy 0.0767\n",
      "Epoch 1 Batch 1350 Loss 2.6924 Accuracy 0.0777\n",
      "Epoch 1 Batch 1400 Loss 2.6735 Accuracy 0.0787\n",
      "Epoch 1 Batch 1450 Loss 2.6566 Accuracy 0.0796\n",
      "Epoch 1 Batch 1500 Loss 2.6398 Accuracy 0.0805\n",
      "Epoch 1 Batch 1550 Loss 2.6235 Accuracy 0.0813\n",
      "Epoch 1 Batch 1600 Loss 2.6090 Accuracy 0.0822\n",
      "Epoch 1 Batch 1650 Loss 2.5955 Accuracy 0.0830\n",
      "Epoch 1 Batch 1700 Loss 2.5821 Accuracy 0.0838\n",
      "Epoch 1 Batch 1750 Loss 2.5699 Accuracy 0.0845\n",
      "Epoch 1 Batch 1800 Loss 2.5577 Accuracy 0.0851\n",
      "Epoch 1 Batch 1850 Loss 2.5467 Accuracy 0.0858\n",
      "Epoch 1 Batch 1900 Loss 2.5362 Accuracy 0.0864\n",
      "Epoch 1 Batch 1950 Loss 2.5263 Accuracy 0.0870\n",
      "Epoch 1 Batch 2000 Loss 2.5160 Accuracy 0.0875\n",
      "Epoch 1 Batch 2050 Loss 2.5066 Accuracy 0.0880\n",
      "Epoch 1 Batch 2100 Loss 2.4984 Accuracy 0.0886\n",
      "Epoch 1 Batch 2150 Loss 2.4895 Accuracy 0.0891\n",
      "Saving checkpoint for epoch 1 at ckpt/ckpt-1\n",
      "Time taken for 1 epoch: 3238.9644253253937 secs\n",
      "\n",
      "Start of epoch 2\n",
      "Epoch 2 Batch 0 Loss 2.2246 Accuracy 0.1003\n",
      "Epoch 2 Batch 50 Loss 2.1419 Accuracy 0.1127\n",
      "Epoch 2 Batch 100 Loss 2.1069 Accuracy 0.1123\n",
      "Epoch 2 Batch 150 Loss 2.0838 Accuracy 0.1124\n",
      "Epoch 2 Batch 200 Loss 2.0890 Accuracy 0.1129\n",
      "Epoch 2 Batch 250 Loss 2.0937 Accuracy 0.1129\n",
      "Epoch 2 Batch 300 Loss 2.0873 Accuracy 0.1130\n",
      "Epoch 2 Batch 350 Loss 2.0903 Accuracy 0.1129\n",
      "Epoch 2 Batch 400 Loss 2.0930 Accuracy 0.1127\n",
      "Epoch 2 Batch 450 Loss 2.0890 Accuracy 0.1127\n",
      "Epoch 2 Batch 500 Loss 2.0884 Accuracy 0.1128\n",
      "Epoch 2 Batch 550 Loss 2.0867 Accuracy 0.1128\n",
      "Epoch 2 Batch 600 Loss 2.0873 Accuracy 0.1130\n",
      "Epoch 2 Batch 650 Loss 2.0870 Accuracy 0.1132\n",
      "Epoch 2 Batch 700 Loss 2.0846 Accuracy 0.1132\n",
      "Epoch 2 Batch 750 Loss 2.0870 Accuracy 0.1133\n",
      "Epoch 2 Batch 800 Loss 2.0820 Accuracy 0.1133\n",
      "Epoch 2 Batch 850 Loss 2.0769 Accuracy 0.1131\n",
      "Epoch 2 Batch 900 Loss 2.0754 Accuracy 0.1131\n",
      "Epoch 2 Batch 950 Loss 2.0722 Accuracy 0.1131\n",
      "Epoch 2 Batch 1000 Loss 2.0709 Accuracy 0.1132\n",
      "Epoch 2 Batch 1050 Loss 2.0683 Accuracy 0.1132\n",
      "Epoch 2 Batch 1100 Loss 2.0642 Accuracy 0.1132\n",
      "Epoch 2 Batch 1150 Loss 2.0635 Accuracy 0.1132\n",
      "Epoch 2 Batch 1200 Loss 2.0609 Accuracy 0.1132\n",
      "Epoch 2 Batch 1250 Loss 2.0610 Accuracy 0.1132\n",
      "Epoch 2 Batch 1300 Loss 2.0612 Accuracy 0.1132\n",
      "Epoch 2 Batch 1350 Loss 2.0588 Accuracy 0.1132\n",
      "Epoch 2 Batch 1400 Loss 2.0561 Accuracy 0.1132\n",
      "Epoch 2 Batch 1450 Loss 2.0542 Accuracy 0.1132\n",
      "Epoch 2 Batch 1500 Loss 2.0559 Accuracy 0.1133\n",
      "Epoch 2 Batch 1550 Loss 2.0537 Accuracy 0.1133\n",
      "Epoch 2 Batch 1600 Loss 2.0530 Accuracy 0.1134\n",
      "Epoch 2 Batch 1650 Loss 2.0540 Accuracy 0.1135\n",
      "Epoch 2 Batch 1700 Loss 2.0526 Accuracy 0.1135\n",
      "Epoch 2 Batch 1750 Loss 2.0517 Accuracy 0.1135\n",
      "Epoch 2 Batch 1800 Loss 2.0509 Accuracy 0.1136\n",
      "Epoch 2 Batch 1850 Loss 2.0512 Accuracy 0.1136\n",
      "Epoch 2 Batch 1900 Loss 2.0511 Accuracy 0.1137\n",
      "Epoch 2 Batch 1950 Loss 2.0505 Accuracy 0.1137\n",
      "Epoch 2 Batch 2000 Loss 2.0511 Accuracy 0.1138\n",
      "Epoch 2 Batch 2050 Loss 2.0508 Accuracy 0.1139\n",
      "Epoch 2 Batch 2100 Loss 2.0503 Accuracy 0.1140\n",
      "Epoch 2 Batch 2150 Loss 2.0498 Accuracy 0.1141\n",
      "Saving checkpoint for epoch 2 at ckpt/ckpt-2\n",
      "Time taken for 1 epoch: 3898.5787019729614 secs\n",
      "\n",
      "Start of epoch 3\n",
      "Epoch 3 Batch 0 Loss 1.9483 Accuracy 0.1184\n",
      "Epoch 3 Batch 50 Loss 2.0041 Accuracy 0.1171\n",
      "Epoch 3 Batch 100 Loss 1.9967 Accuracy 0.1170\n",
      "Epoch 3 Batch 150 Loss 2.0075 Accuracy 0.1179\n",
      "Epoch 3 Batch 200 Loss 1.9919 Accuracy 0.1174\n",
      "Epoch 3 Batch 250 Loss 1.9900 Accuracy 0.1177\n",
      "Epoch 3 Batch 300 Loss 1.9895 Accuracy 0.1176\n",
      "Epoch 3 Batch 350 Loss 1.9887 Accuracy 0.1179\n",
      "Epoch 3 Batch 400 Loss 1.9897 Accuracy 0.1179\n",
      "Epoch 3 Batch 450 Loss 1.9924 Accuracy 0.1180\n",
      "Epoch 3 Batch 500 Loss 1.9976 Accuracy 0.1180\n",
      "Epoch 3 Batch 550 Loss 1.9966 Accuracy 0.1182\n",
      "Epoch 3 Batch 600 Loss 1.9959 Accuracy 0.1185\n",
      "Epoch 3 Batch 650 Loss 1.9963 Accuracy 0.1187\n",
      "Epoch 3 Batch 700 Loss 1.9973 Accuracy 0.1186\n",
      "Epoch 3 Batch 750 Loss 1.9954 Accuracy 0.1186\n",
      "Epoch 3 Batch 800 Loss 1.9917 Accuracy 0.1186\n",
      "Epoch 3 Batch 850 Loss 1.9905 Accuracy 0.1187\n",
      "Epoch 3 Batch 900 Loss 1.9893 Accuracy 0.1188\n",
      "Epoch 3 Batch 950 Loss 1.9871 Accuracy 0.1188\n",
      "Epoch 3 Batch 1000 Loss 1.9875 Accuracy 0.1187\n",
      "Epoch 3 Batch 1050 Loss 1.9840 Accuracy 0.1187\n",
      "Epoch 3 Batch 1100 Loss 1.9838 Accuracy 0.1187\n",
      "Epoch 3 Batch 1150 Loss 1.9809 Accuracy 0.1187\n",
      "Epoch 3 Batch 1200 Loss 1.9795 Accuracy 0.1186\n",
      "Epoch 3 Batch 1250 Loss 1.9786 Accuracy 0.1187\n",
      "Epoch 3 Batch 1300 Loss 1.9776 Accuracy 0.1187\n",
      "Epoch 3 Batch 1350 Loss 1.9765 Accuracy 0.1187\n",
      "Epoch 3 Batch 1400 Loss 1.9747 Accuracy 0.1188\n",
      "Epoch 3 Batch 1450 Loss 1.9744 Accuracy 0.1189\n",
      "Epoch 3 Batch 1500 Loss 1.9748 Accuracy 0.1190\n",
      "Epoch 3 Batch 1550 Loss 1.9737 Accuracy 0.1191\n",
      "Epoch 3 Batch 1600 Loss 1.9730 Accuracy 0.1191\n",
      "Epoch 3 Batch 1650 Loss 1.9726 Accuracy 0.1193\n",
      "Epoch 3 Batch 1700 Loss 1.9715 Accuracy 0.1192\n",
      "Epoch 3 Batch 1750 Loss 1.9713 Accuracy 0.1193\n",
      "Epoch 3 Batch 1800 Loss 1.9713 Accuracy 0.1193\n",
      "Epoch 3 Batch 1850 Loss 1.9710 Accuracy 0.1194\n",
      "Epoch 3 Batch 1900 Loss 1.9709 Accuracy 0.1194\n",
      "Epoch 3 Batch 1950 Loss 1.9715 Accuracy 0.1195\n",
      "Epoch 3 Batch 2000 Loss 1.9721 Accuracy 0.1196\n",
      "Epoch 3 Batch 2050 Loss 1.9717 Accuracy 0.1196\n",
      "Epoch 3 Batch 2100 Loss 1.9716 Accuracy 0.1197\n",
      "Epoch 3 Batch 2150 Loss 1.9708 Accuracy 0.1197\n",
      "Saving checkpoint for epoch 3 at ckpt/ckpt-3\n",
      "Time taken for 1 epoch: 3031.541385412216 secs\n",
      "\n",
      "Start of epoch 4\n",
      "Epoch 4 Batch 0 Loss 1.9795 Accuracy 0.1192\n",
      "Epoch 4 Batch 50 Loss 1.9073 Accuracy 0.1217\n",
      "Epoch 4 Batch 100 Loss 1.9004 Accuracy 0.1230\n",
      "Epoch 4 Batch 150 Loss 1.9109 Accuracy 0.1229\n",
      "Epoch 4 Batch 200 Loss 1.9155 Accuracy 0.1236\n",
      "Epoch 4 Batch 250 Loss 1.9170 Accuracy 0.1231\n",
      "Epoch 4 Batch 300 Loss 1.9165 Accuracy 0.1232\n",
      "Epoch 4 Batch 350 Loss 1.9218 Accuracy 0.1233\n",
      "Epoch 4 Batch 400 Loss 1.9185 Accuracy 0.1234\n",
      "Epoch 4 Batch 450 Loss 1.9201 Accuracy 0.1235\n",
      "Epoch 4 Batch 500 Loss 1.9222 Accuracy 0.1234\n",
      "Epoch 4 Batch 550 Loss 1.9208 Accuracy 0.1235\n",
      "Epoch 4 Batch 600 Loss 1.9226 Accuracy 0.1234\n",
      "Epoch 4 Batch 650 Loss 1.9272 Accuracy 0.1235\n",
      "Epoch 4 Batch 700 Loss 1.9256 Accuracy 0.1236\n",
      "Epoch 4 Batch 750 Loss 1.9235 Accuracy 0.1236\n",
      "Epoch 4 Batch 800 Loss 1.9229 Accuracy 0.1236\n",
      "Epoch 4 Batch 850 Loss 1.9229 Accuracy 0.1237\n",
      "Epoch 4 Batch 900 Loss 1.9202 Accuracy 0.1236\n",
      "Epoch 4 Batch 950 Loss 1.9197 Accuracy 0.1236\n",
      "Epoch 4 Batch 1000 Loss 1.9198 Accuracy 0.1237\n",
      "Epoch 4 Batch 1050 Loss 1.9187 Accuracy 0.1236\n",
      "Epoch 4 Batch 1100 Loss 1.9159 Accuracy 0.1236\n",
      "Epoch 4 Batch 1150 Loss 1.9168 Accuracy 0.1236\n",
      "Epoch 4 Batch 1200 Loss 1.9174 Accuracy 0.1236\n",
      "Epoch 4 Batch 1250 Loss 1.9160 Accuracy 0.1235\n",
      "Epoch 4 Batch 1300 Loss 1.9133 Accuracy 0.1234\n",
      "Epoch 4 Batch 1350 Loss 1.9114 Accuracy 0.1234\n",
      "Epoch 4 Batch 1400 Loss 1.9097 Accuracy 0.1234\n",
      "Epoch 4 Batch 1450 Loss 1.9110 Accuracy 0.1234\n",
      "Epoch 4 Batch 1500 Loss 1.9107 Accuracy 0.1235\n",
      "Epoch 4 Batch 1550 Loss 1.9105 Accuracy 0.1234\n",
      "Epoch 4 Batch 1600 Loss 1.9100 Accuracy 0.1235\n",
      "Epoch 4 Batch 1650 Loss 1.9089 Accuracy 0.1235\n",
      "Epoch 4 Batch 1700 Loss 1.9103 Accuracy 0.1236\n",
      "Epoch 4 Batch 1750 Loss 1.9106 Accuracy 0.1236\n",
      "Epoch 4 Batch 1800 Loss 1.9096 Accuracy 0.1237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch 1850 Loss 1.9109 Accuracy 0.1238\n",
      "Epoch 4 Batch 1900 Loss 1.9112 Accuracy 0.1238\n",
      "Epoch 4 Batch 1950 Loss 1.9118 Accuracy 0.1238\n",
      "Epoch 4 Batch 2000 Loss 1.9120 Accuracy 0.1239\n",
      "Epoch 4 Batch 2050 Loss 1.9127 Accuracy 0.1240\n",
      "Epoch 4 Batch 2100 Loss 1.9127 Accuracy 0.1240\n",
      "Epoch 4 Batch 2150 Loss 1.9132 Accuracy 0.1240\n",
      "Saving checkpoint for epoch 4 at ckpt/ckpt-4\n",
      "Time taken for 1 epoch: 3612.9534027576447 secs\n",
      "\n",
      "Start of epoch 5\n",
      "Epoch 5 Batch 0 Loss 2.1080 Accuracy 0.1308\n",
      "Epoch 5 Batch 50 Loss 1.9074 Accuracy 0.1253\n",
      "Epoch 5 Batch 100 Loss 1.8713 Accuracy 0.1254\n",
      "Epoch 5 Batch 150 Loss 1.8663 Accuracy 0.1257\n",
      "Epoch 5 Batch 200 Loss 1.8746 Accuracy 0.1265\n",
      "Epoch 5 Batch 250 Loss 1.8693 Accuracy 0.1264\n",
      "Epoch 5 Batch 300 Loss 1.8710 Accuracy 0.1265\n",
      "Epoch 5 Batch 350 Loss 1.8729 Accuracy 0.1262\n",
      "Epoch 5 Batch 400 Loss 1.8780 Accuracy 0.1263\n",
      "Epoch 5 Batch 450 Loss 1.8807 Accuracy 0.1261\n",
      "Epoch 5 Batch 500 Loss 1.8843 Accuracy 0.1261\n",
      "Epoch 5 Batch 550 Loss 1.8815 Accuracy 0.1261\n",
      "Epoch 5 Batch 600 Loss 1.8840 Accuracy 0.1265\n",
      "Epoch 5 Batch 650 Loss 1.8830 Accuracy 0.1263\n",
      "Epoch 5 Batch 700 Loss 1.8856 Accuracy 0.1264\n",
      "Epoch 5 Batch 750 Loss 1.8853 Accuracy 0.1266\n",
      "Epoch 5 Batch 800 Loss 1.8835 Accuracy 0.1265\n",
      "Epoch 5 Batch 850 Loss 1.8813 Accuracy 0.1266\n",
      "Epoch 5 Batch 900 Loss 1.8802 Accuracy 0.1266\n",
      "Epoch 5 Batch 950 Loss 1.8775 Accuracy 0.1267\n",
      "Epoch 5 Batch 1000 Loss 1.8775 Accuracy 0.1267\n",
      "Epoch 5 Batch 1050 Loss 1.8752 Accuracy 0.1267\n",
      "Epoch 5 Batch 1100 Loss 1.8747 Accuracy 0.1265\n",
      "Epoch 5 Batch 1150 Loss 1.8729 Accuracy 0.1265\n",
      "Epoch 5 Batch 1200 Loss 1.8730 Accuracy 0.1265\n",
      "Epoch 5 Batch 1250 Loss 1.8741 Accuracy 0.1266\n",
      "Epoch 5 Batch 1300 Loss 1.8727 Accuracy 0.1266\n",
      "Epoch 5 Batch 1350 Loss 1.8711 Accuracy 0.1265\n",
      "Epoch 5 Batch 1400 Loss 1.8686 Accuracy 0.1265\n",
      "Epoch 5 Batch 1450 Loss 1.8687 Accuracy 0.1266\n",
      "Epoch 5 Batch 1500 Loss 1.8689 Accuracy 0.1266\n",
      "Epoch 5 Batch 1550 Loss 1.8686 Accuracy 0.1266\n",
      "Epoch 5 Batch 1600 Loss 1.8688 Accuracy 0.1267\n",
      "Epoch 5 Batch 1650 Loss 1.8692 Accuracy 0.1268\n",
      "Epoch 5 Batch 1700 Loss 1.8681 Accuracy 0.1268\n",
      "Epoch 5 Batch 1750 Loss 1.8683 Accuracy 0.1268\n",
      "Epoch 5 Batch 1800 Loss 1.8689 Accuracy 0.1269\n",
      "Epoch 5 Batch 1850 Loss 1.8687 Accuracy 0.1269\n",
      "Epoch 5 Batch 1900 Loss 1.8692 Accuracy 0.1270\n",
      "Epoch 5 Batch 1950 Loss 1.8696 Accuracy 0.1270\n",
      "Epoch 5 Batch 2000 Loss 1.8690 Accuracy 0.1270\n",
      "Epoch 5 Batch 2050 Loss 1.8705 Accuracy 0.1271\n",
      "Epoch 5 Batch 2100 Loss 1.8716 Accuracy 0.1271\n",
      "Epoch 5 Batch 2150 Loss 1.8715 Accuracy 0.1271\n",
      "Saving checkpoint for epoch 5 at ckpt/ckpt-5\n",
      "Time taken for 1 epoch: 3615.1939907073975 secs\n",
      "\n",
      "Start of epoch 6\n",
      "Epoch 6 Batch 0 Loss 1.7664 Accuracy 0.1406\n",
      "Epoch 6 Batch 50 Loss 1.8840 Accuracy 0.1283\n",
      "Epoch 6 Batch 100 Loss 1.8396 Accuracy 0.1282\n",
      "Epoch 6 Batch 150 Loss 1.8464 Accuracy 0.1289\n",
      "Epoch 6 Batch 200 Loss 1.8378 Accuracy 0.1289\n",
      "Epoch 6 Batch 250 Loss 1.8394 Accuracy 0.1290\n",
      "Epoch 6 Batch 300 Loss 1.8433 Accuracy 0.1294\n",
      "Epoch 6 Batch 350 Loss 1.8514 Accuracy 0.1295\n",
      "Epoch 6 Batch 400 Loss 1.8515 Accuracy 0.1294\n",
      "Epoch 6 Batch 450 Loss 1.8513 Accuracy 0.1291\n",
      "Epoch 6 Batch 500 Loss 1.8513 Accuracy 0.1293\n",
      "Epoch 6 Batch 550 Loss 1.8501 Accuracy 0.1294\n",
      "Epoch 6 Batch 600 Loss 1.8494 Accuracy 0.1293\n",
      "Epoch 6 Batch 650 Loss 1.8512 Accuracy 0.1295\n",
      "Epoch 6 Batch 700 Loss 1.8544 Accuracy 0.1295\n",
      "Epoch 6 Batch 750 Loss 1.8551 Accuracy 0.1295\n",
      "Epoch 6 Batch 800 Loss 1.8521 Accuracy 0.1295\n",
      "Epoch 6 Batch 850 Loss 1.8483 Accuracy 0.1295\n",
      "Epoch 6 Batch 900 Loss 1.8429 Accuracy 0.1294\n",
      "Epoch 6 Batch 950 Loss 1.8408 Accuracy 0.1293\n",
      "Epoch 6 Batch 1000 Loss 1.8402 Accuracy 0.1292\n",
      "Epoch 6 Batch 1050 Loss 1.8397 Accuracy 0.1292\n",
      "Epoch 6 Batch 1100 Loss 1.8377 Accuracy 0.1293\n",
      "Epoch 6 Batch 1150 Loss 1.8371 Accuracy 0.1292\n",
      "Epoch 6 Batch 1200 Loss 1.8355 Accuracy 0.1292\n",
      "Epoch 6 Batch 1250 Loss 1.8358 Accuracy 0.1292\n",
      "Epoch 6 Batch 1300 Loss 1.8347 Accuracy 0.1292\n",
      "Epoch 6 Batch 1350 Loss 1.8337 Accuracy 0.1292\n",
      "Epoch 6 Batch 1400 Loss 1.8338 Accuracy 0.1292\n",
      "Epoch 6 Batch 1450 Loss 1.8345 Accuracy 0.1292\n",
      "Epoch 6 Batch 1500 Loss 1.8352 Accuracy 0.1292\n",
      "Epoch 6 Batch 1550 Loss 1.8340 Accuracy 0.1291\n",
      "Epoch 6 Batch 1600 Loss 1.8333 Accuracy 0.1292\n",
      "Epoch 6 Batch 1650 Loss 1.8343 Accuracy 0.1292\n",
      "Epoch 6 Batch 1700 Loss 1.8337 Accuracy 0.1292\n",
      "Epoch 6 Batch 1750 Loss 1.8343 Accuracy 0.1293\n",
      "Epoch 6 Batch 1800 Loss 1.8349 Accuracy 0.1293\n",
      "Epoch 6 Batch 1850 Loss 1.8358 Accuracy 0.1293\n",
      "Epoch 6 Batch 1900 Loss 1.8354 Accuracy 0.1293\n",
      "Epoch 6 Batch 1950 Loss 1.8354 Accuracy 0.1293\n",
      "Epoch 6 Batch 2000 Loss 1.8368 Accuracy 0.1293\n",
      "Epoch 6 Batch 2050 Loss 1.8385 Accuracy 0.1293\n",
      "Epoch 6 Batch 2100 Loss 1.8387 Accuracy 0.1293\n",
      "Epoch 6 Batch 2150 Loss 1.8394 Accuracy 0.1294\n",
      "Saving checkpoint for epoch 6 at ckpt/ckpt-6\n",
      "Time taken for 1 epoch: 3846.7519075870514 secs\n",
      "\n",
      "Start of epoch 7\n",
      "Epoch 7 Batch 0 Loss 1.5944 Accuracy 0.1242\n",
      "Epoch 7 Batch 50 Loss 1.7975 Accuracy 0.1318\n",
      "Epoch 7 Batch 100 Loss 1.7997 Accuracy 0.1318\n",
      "Epoch 7 Batch 150 Loss 1.8089 Accuracy 0.1324\n",
      "Epoch 7 Batch 200 Loss 1.8059 Accuracy 0.1322\n",
      "Epoch 7 Batch 250 Loss 1.8089 Accuracy 0.1327\n",
      "Epoch 7 Batch 300 Loss 1.8125 Accuracy 0.1323\n",
      "Epoch 7 Batch 350 Loss 1.8114 Accuracy 0.1321\n",
      "Epoch 7 Batch 400 Loss 1.8111 Accuracy 0.1323\n",
      "Epoch 7 Batch 450 Loss 1.8147 Accuracy 0.1320\n",
      "Epoch 7 Batch 500 Loss 1.8154 Accuracy 0.1319\n",
      "Epoch 7 Batch 550 Loss 1.8171 Accuracy 0.1317\n",
      "Epoch 7 Batch 600 Loss 1.8156 Accuracy 0.1315\n",
      "Epoch 7 Batch 650 Loss 1.8170 Accuracy 0.1317\n",
      "Epoch 7 Batch 700 Loss 1.8185 Accuracy 0.1317\n",
      "Epoch 7 Batch 750 Loss 1.8186 Accuracy 0.1316\n",
      "Epoch 7 Batch 800 Loss 1.8175 Accuracy 0.1317\n",
      "Epoch 7 Batch 850 Loss 1.8144 Accuracy 0.1317\n",
      "Epoch 7 Batch 900 Loss 1.8120 Accuracy 0.1315\n",
      "Epoch 7 Batch 950 Loss 1.8107 Accuracy 0.1315\n",
      "Epoch 7 Batch 1000 Loss 1.8101 Accuracy 0.1314\n",
      "Epoch 7 Batch 1050 Loss 1.8090 Accuracy 0.1314\n",
      "Epoch 7 Batch 1100 Loss 1.8104 Accuracy 0.1314\n",
      "Epoch 7 Batch 1150 Loss 1.8118 Accuracy 0.1315\n",
      "Epoch 7 Batch 1200 Loss 1.8114 Accuracy 0.1313\n",
      "Epoch 7 Batch 1250 Loss 1.8134 Accuracy 0.1315\n",
      "Epoch 7 Batch 1300 Loss 1.8132 Accuracy 0.1315\n",
      "Epoch 7 Batch 1350 Loss 1.8120 Accuracy 0.1313\n",
      "Epoch 7 Batch 1400 Loss 1.8115 Accuracy 0.1313\n",
      "Epoch 7 Batch 1450 Loss 1.8105 Accuracy 0.1313\n",
      "Epoch 7 Batch 1500 Loss 1.8104 Accuracy 0.1313\n",
      "Epoch 7 Batch 1550 Loss 1.8094 Accuracy 0.1312\n",
      "Epoch 7 Batch 1600 Loss 1.8096 Accuracy 0.1312\n",
      "Epoch 7 Batch 1650 Loss 1.8099 Accuracy 0.1312\n",
      "Epoch 7 Batch 1700 Loss 1.8110 Accuracy 0.1313\n",
      "Epoch 7 Batch 1750 Loss 1.8110 Accuracy 0.1312\n",
      "Epoch 7 Batch 1800 Loss 1.8111 Accuracy 0.1313\n",
      "Epoch 7 Batch 1850 Loss 1.8111 Accuracy 0.1313\n",
      "Epoch 7 Batch 1900 Loss 1.8120 Accuracy 0.1313\n",
      "Epoch 7 Batch 1950 Loss 1.8119 Accuracy 0.1313\n",
      "Epoch 7 Batch 2000 Loss 1.8132 Accuracy 0.1313\n",
      "Epoch 7 Batch 2050 Loss 1.8136 Accuracy 0.1313\n",
      "Epoch 7 Batch 2100 Loss 1.8146 Accuracy 0.1313\n",
      "Epoch 7 Batch 2150 Loss 1.8150 Accuracy 0.1314\n",
      "Saving checkpoint for epoch 7 at ckpt/ckpt-7\n",
      "Time taken for 1 epoch: 4019.255207300186 secs\n",
      "\n",
      "Start of epoch 8\n",
      "Epoch 8 Batch 0 Loss 1.8649 Accuracy 0.1382\n",
      "Epoch 8 Batch 50 Loss 1.7947 Accuracy 0.1321\n",
      "Epoch 8 Batch 100 Loss 1.7817 Accuracy 0.1328\n",
      "Epoch 8 Batch 150 Loss 1.7855 Accuracy 0.1324\n",
      "Epoch 8 Batch 200 Loss 1.7953 Accuracy 0.1329\n",
      "Epoch 8 Batch 250 Loss 1.7921 Accuracy 0.1330\n",
      "Epoch 8 Batch 300 Loss 1.7968 Accuracy 0.1332\n",
      "Epoch 8 Batch 350 Loss 1.7983 Accuracy 0.1333\n",
      "Epoch 8 Batch 400 Loss 1.7955 Accuracy 0.1333\n",
      "Epoch 8 Batch 450 Loss 1.7969 Accuracy 0.1331\n",
      "Epoch 8 Batch 500 Loss 1.7989 Accuracy 0.1331\n",
      "Epoch 8 Batch 550 Loss 1.8042 Accuracy 0.1333\n",
      "Epoch 8 Batch 600 Loss 1.8013 Accuracy 0.1333\n",
      "Epoch 8 Batch 650 Loss 1.8026 Accuracy 0.1334\n",
      "Epoch 8 Batch 700 Loss 1.8027 Accuracy 0.1331\n",
      "Epoch 8 Batch 750 Loss 1.8029 Accuracy 0.1331\n",
      "Epoch 8 Batch 800 Loss 1.8010 Accuracy 0.1330\n",
      "Epoch 8 Batch 850 Loss 1.7993 Accuracy 0.1330\n",
      "Epoch 8 Batch 900 Loss 1.7949 Accuracy 0.1329\n",
      "Epoch 8 Batch 950 Loss 1.7935 Accuracy 0.1329\n",
      "Epoch 8 Batch 1000 Loss 1.7930 Accuracy 0.1331\n",
      "Epoch 8 Batch 1050 Loss 1.7925 Accuracy 0.1330\n",
      "Epoch 8 Batch 1100 Loss 1.7901 Accuracy 0.1328\n",
      "Epoch 8 Batch 1150 Loss 1.7904 Accuracy 0.1328\n",
      "Epoch 8 Batch 1200 Loss 1.7895 Accuracy 0.1328\n",
      "Epoch 8 Batch 1250 Loss 1.7910 Accuracy 0.1327\n",
      "Epoch 8 Batch 1300 Loss 1.7908 Accuracy 0.1327\n",
      "Epoch 8 Batch 1350 Loss 1.7906 Accuracy 0.1326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Batch 1400 Loss 1.7889 Accuracy 0.1326\n",
      "Epoch 8 Batch 1450 Loss 1.7894 Accuracy 0.1326\n",
      "Epoch 8 Batch 1500 Loss 1.7892 Accuracy 0.1326\n",
      "Epoch 8 Batch 1550 Loss 1.7886 Accuracy 0.1326\n",
      "Epoch 8 Batch 1600 Loss 1.7901 Accuracy 0.1326\n",
      "Epoch 8 Batch 1650 Loss 1.7916 Accuracy 0.1327\n",
      "Epoch 8 Batch 1700 Loss 1.7910 Accuracy 0.1327\n",
      "Epoch 8 Batch 1750 Loss 1.7920 Accuracy 0.1328\n",
      "Epoch 8 Batch 1800 Loss 1.7926 Accuracy 0.1328\n",
      "Epoch 8 Batch 1850 Loss 1.7924 Accuracy 0.1328\n",
      "Epoch 8 Batch 1900 Loss 1.7932 Accuracy 0.1328\n",
      "Epoch 8 Batch 1950 Loss 1.7933 Accuracy 0.1328\n",
      "Epoch 8 Batch 2000 Loss 1.7935 Accuracy 0.1327\n",
      "Epoch 8 Batch 2050 Loss 1.7945 Accuracy 0.1328\n",
      "Epoch 8 Batch 2100 Loss 1.7947 Accuracy 0.1328\n",
      "Epoch 8 Batch 2150 Loss 1.7956 Accuracy 0.1329\n",
      "Saving checkpoint for epoch 8 at ckpt/ckpt-8\n",
      "Time taken for 1 epoch: 3750.725405216217 secs\n",
      "\n",
      "Start of epoch 9\n",
      "Epoch 9 Batch 0 Loss 1.8427 Accuracy 0.1464\n",
      "Epoch 9 Batch 50 Loss 1.7693 Accuracy 0.1342\n",
      "Epoch 9 Batch 100 Loss 1.7726 Accuracy 0.1350\n",
      "Epoch 9 Batch 150 Loss 1.7672 Accuracy 0.1349\n",
      "Epoch 9 Batch 200 Loss 1.7683 Accuracy 0.1350\n",
      "Epoch 9 Batch 250 Loss 1.7623 Accuracy 0.1347\n",
      "Epoch 9 Batch 300 Loss 1.7659 Accuracy 0.1346\n",
      "Epoch 9 Batch 350 Loss 1.7679 Accuracy 0.1351\n",
      "Epoch 9 Batch 400 Loss 1.7739 Accuracy 0.1349\n",
      "Epoch 9 Batch 450 Loss 1.7768 Accuracy 0.1349\n",
      "Epoch 9 Batch 500 Loss 1.7801 Accuracy 0.1347\n",
      "Epoch 9 Batch 550 Loss 1.7786 Accuracy 0.1348\n",
      "Epoch 9 Batch 600 Loss 1.7805 Accuracy 0.1348\n",
      "Epoch 9 Batch 650 Loss 1.7807 Accuracy 0.1348\n",
      "Epoch 9 Batch 700 Loss 1.7848 Accuracy 0.1350\n",
      "Epoch 9 Batch 750 Loss 1.7836 Accuracy 0.1350\n",
      "Epoch 9 Batch 800 Loss 1.7829 Accuracy 0.1347\n",
      "Epoch 9 Batch 850 Loss 1.7804 Accuracy 0.1347\n",
      "Epoch 9 Batch 900 Loss 1.7806 Accuracy 0.1347\n",
      "Epoch 9 Batch 950 Loss 1.7781 Accuracy 0.1347\n",
      "Epoch 9 Batch 1000 Loss 1.7771 Accuracy 0.1345\n",
      "Epoch 9 Batch 1050 Loss 1.7756 Accuracy 0.1345\n",
      "Epoch 9 Batch 1100 Loss 1.7765 Accuracy 0.1344\n",
      "Epoch 9 Batch 1150 Loss 1.7773 Accuracy 0.1344\n",
      "Epoch 9 Batch 1200 Loss 1.7774 Accuracy 0.1345\n",
      "Epoch 9 Batch 1250 Loss 1.7778 Accuracy 0.1344\n",
      "Epoch 9 Batch 1300 Loss 1.7766 Accuracy 0.1343\n",
      "Epoch 9 Batch 1350 Loss 1.7769 Accuracy 0.1343\n",
      "Epoch 9 Batch 1400 Loss 1.7747 Accuracy 0.1342\n",
      "Epoch 9 Batch 1450 Loss 1.7739 Accuracy 0.1342\n",
      "Epoch 9 Batch 1500 Loss 1.7742 Accuracy 0.1342\n",
      "Epoch 9 Batch 1550 Loss 1.7740 Accuracy 0.1341\n",
      "Epoch 9 Batch 1600 Loss 1.7750 Accuracy 0.1342\n",
      "Epoch 9 Batch 1650 Loss 1.7752 Accuracy 0.1343\n",
      "Epoch 9 Batch 1700 Loss 1.7759 Accuracy 0.1342\n",
      "Epoch 9 Batch 1750 Loss 1.7759 Accuracy 0.1341\n",
      "Epoch 9 Batch 1800 Loss 1.7762 Accuracy 0.1342\n",
      "Epoch 9 Batch 1850 Loss 1.7769 Accuracy 0.1342\n",
      "Epoch 9 Batch 1900 Loss 1.7779 Accuracy 0.1341\n",
      "Epoch 9 Batch 1950 Loss 1.7783 Accuracy 0.1341\n",
      "Epoch 9 Batch 2000 Loss 1.7786 Accuracy 0.1341\n",
      "Epoch 9 Batch 2050 Loss 1.7792 Accuracy 0.1342\n",
      "Epoch 9 Batch 2100 Loss 1.7793 Accuracy 0.1342\n",
      "Epoch 9 Batch 2150 Loss 1.7801 Accuracy 0.1343\n",
      "Saving checkpoint for epoch 9 at ckpt/ckpt-9\n",
      "Time taken for 1 epoch: 2784.980409860611 secs\n",
      "\n",
      "Start of epoch 10\n",
      "Epoch 10 Batch 0 Loss 1.8941 Accuracy 0.1299\n",
      "Epoch 10 Batch 50 Loss 1.7596 Accuracy 0.1326\n",
      "Epoch 10 Batch 100 Loss 1.7435 Accuracy 0.1339\n",
      "Epoch 10 Batch 150 Loss 1.7312 Accuracy 0.1335\n",
      "Epoch 10 Batch 200 Loss 1.7356 Accuracy 0.1341\n",
      "Epoch 10 Batch 250 Loss 1.7450 Accuracy 0.1346\n",
      "Epoch 10 Batch 300 Loss 1.7468 Accuracy 0.1348\n",
      "Epoch 10 Batch 350 Loss 1.7519 Accuracy 0.1353\n",
      "Epoch 10 Batch 400 Loss 1.7542 Accuracy 0.1352\n",
      "Epoch 10 Batch 450 Loss 1.7569 Accuracy 0.1351\n",
      "Epoch 10 Batch 500 Loss 1.7588 Accuracy 0.1352\n",
      "Epoch 10 Batch 550 Loss 1.7576 Accuracy 0.1353\n",
      "Epoch 10 Batch 600 Loss 1.7590 Accuracy 0.1354\n",
      "Epoch 10 Batch 650 Loss 1.7603 Accuracy 0.1356\n",
      "Epoch 10 Batch 700 Loss 1.7646 Accuracy 0.1358\n",
      "Epoch 10 Batch 750 Loss 1.7643 Accuracy 0.1359\n",
      "Epoch 10 Batch 800 Loss 1.7624 Accuracy 0.1358\n",
      "Epoch 10 Batch 850 Loss 1.7626 Accuracy 0.1357\n",
      "Epoch 10 Batch 900 Loss 1.7618 Accuracy 0.1357\n",
      "Epoch 10 Batch 950 Loss 1.7615 Accuracy 0.1355\n",
      "Epoch 10 Batch 1000 Loss 1.7606 Accuracy 0.1354\n",
      "Epoch 10 Batch 1050 Loss 1.7587 Accuracy 0.1353\n",
      "Epoch 10 Batch 1100 Loss 1.7594 Accuracy 0.1353\n",
      "Epoch 10 Batch 1150 Loss 1.7605 Accuracy 0.1352\n",
      "Epoch 10 Batch 1200 Loss 1.7605 Accuracy 0.1351\n",
      "Epoch 10 Batch 1250 Loss 1.7601 Accuracy 0.1350\n",
      "Epoch 10 Batch 1300 Loss 1.7597 Accuracy 0.1350\n",
      "Epoch 10 Batch 1350 Loss 1.7602 Accuracy 0.1350\n",
      "Epoch 10 Batch 1400 Loss 1.7608 Accuracy 0.1350\n",
      "Epoch 10 Batch 1450 Loss 1.7612 Accuracy 0.1349\n",
      "Epoch 10 Batch 1500 Loss 1.7613 Accuracy 0.1349\n",
      "Epoch 10 Batch 1550 Loss 1.7608 Accuracy 0.1349\n",
      "Epoch 10 Batch 1600 Loss 1.7611 Accuracy 0.1350\n",
      "Epoch 10 Batch 1650 Loss 1.7609 Accuracy 0.1350\n",
      "Epoch 10 Batch 1700 Loss 1.7613 Accuracy 0.1350\n",
      "Epoch 10 Batch 1750 Loss 1.7615 Accuracy 0.1350\n",
      "Epoch 10 Batch 1800 Loss 1.7622 Accuracy 0.1350\n",
      "Epoch 10 Batch 1850 Loss 1.7634 Accuracy 0.1350\n",
      "Epoch 10 Batch 1900 Loss 1.7640 Accuracy 0.1351\n",
      "Epoch 10 Batch 1950 Loss 1.7643 Accuracy 0.1351\n",
      "Epoch 10 Batch 2000 Loss 1.7644 Accuracy 0.1351\n",
      "Epoch 10 Batch 2050 Loss 1.7658 Accuracy 0.1353\n",
      "Epoch 10 Batch 2100 Loss 1.7668 Accuracy 0.1353\n",
      "Epoch 10 Batch 2150 Loss 1.7676 Accuracy 0.1353\n",
      "Saving checkpoint for epoch 10 at ckpt/ckpt-10\n",
      "Time taken for 1 epoch: 2957.5041332244873 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"Start of epoch {}\".format(epoch+1))\n",
    "    start = time.time()\n",
    "    \n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    \n",
    "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
    "        dec_inputs = targets[:, :-1]\n",
    "        dec_outputs_real = targets[:, 1:]\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
    "            loss = loss_function(dec_outputs_real, predictions)\n",
    "        \n",
    "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "        \n",
    "        train_loss(loss)\n",
    "        train_accuracy(dec_outputs_real, predictions)\n",
    "        \n",
    "        if batch % 50 == 0:\n",
    "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
    "                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
    "            \n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1,\n",
    "                                                        ckpt_save_path))\n",
    "    print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    inp_sentence = \\\n",
    "        [VOCAB_SIZE_QST-2] + tokenizer_qst.encode(inp_sentence) + [VOCAB_SIZE_QST-1]\n",
    "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
    "    \n",
    "    output = tf.expand_dims([VOCAB_SIZE_ANS-2], axis=0)\n",
    "    \n",
    "    for _ in range(MAX_LENGTH):\n",
    "        predictions = transformer(enc_input, output, False)\n",
    "        \n",
    "        prediction = predictions[:, -1:, :]\n",
    "        \n",
    "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
    "        \n",
    "        if predicted_id == VOCAB_SIZE_ANS-1:\n",
    "            return tf.squeeze(output, axis=0)\n",
    "        \n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "        \n",
    "    return tf.squeeze(output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(sentence):\n",
    "    output = evaluate(sentence).numpy()\n",
    "    \n",
    "    predicted_sentence = tokenizer_ans.decode(\n",
    "        [i for i in output if i < VOCAB_SIZE_ANS-2]\n",
    "    )\n",
    "    \n",
    "    print(\"Input: {}\".format(sentence))\n",
    "    print(\"Predicted reply: {}\".format(predicted_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: hi\n",
      "Predicted reply: i am not going to be here until i get back\n"
     ]
    }
   ],
   "source": [
    "chatbot(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: what is your name\n",
      "Predicted reply: i am not going to be here\n"
     ]
    }
   ],
   "source": [
    "chatbot(\"what is your name\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
