# -*- coding: utf-8 -*-
"""submission1-fake-news.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nbNXICuChryXgza2AFAwkGFSEfuScrhW

# Welcome to Fake News Classification

by: I Gusti Bagus Awienandra

### Import all dependencies
"""

pip install pydot==1.3.0

pip install graphviz==0.10.1

import tensorflow as tf
import pandas as pd
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
from keras import callbacks
from keras.utils import plot_model
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from keras.regularizers import l2

"""### Import Dataset"""

df = pd.read_csv('fake_or_real_news.csv')

df.info()

df = df[:2001]

df.tail()

"""### Preprocessing"""

category = pd.get_dummies(df.label)

category_array = category.columns.tolist()

df_baru = pd.concat([df, category], axis=1)

df_baru = df_baru.drop(columns='label')

df_baru

sentence = df_baru['text'].values
label = df_baru[category_array].values
train_sentence, test_sentence, train_label, test_label = train_test_split(sentence, label, test_size=0.2)

"""### Create Tokenizer"""

tokenizer = Tokenizer(num_words=5000, split=' ')

"""### Fit Sentence"""

tokenizer.fit_on_texts(train_sentence) 
tokenizer.fit_on_texts(test_sentence)

"""### Convert Text to Index"""

train_sequences = tokenizer.texts_to_sequences(train_sentence)
test_sequences = tokenizer.texts_to_sequences(test_sentence)

vocab_size = len(tokenizer.word_index)+1

"""### Padding"""

train_padded = pad_sequences(train_sequences) 
test_padded = pad_sequences(test_sequences)

"""### Create Model"""

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=16),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=l2(0.001)),
    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=l2(0.001)),
    tf.keras.layers.Dense(2, activation='softmax')
])

model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

print(model.summary())

plot_model(model, to_file='submission1-fake-news.png')

"""### Create Callbacks"""

class myCallback(callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.95):
      print("\nYour Accuracy >95%!")
      self.model.stop_training = True

callbacks = myCallback()

"""### Fit Model"""

history = model.fit(train_padded, train_label, epochs=10, 
                    validation_data=(test_padded, test_label), verbose=2, callbacks=[callbacks], batch_size = 128)

"""### Plotting

#### Plot Model Loss
"""

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""#### Plot Model Accuracy"""

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Akurasi Model')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

