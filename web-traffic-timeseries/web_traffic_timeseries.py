# -*- coding: utf-8 -*-
"""submission2_web_traffic_timeseries.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uIQV0HGWxUg1uglIdGSWmKZZvHX5ot0e

# Welcome to Web Traffic Timeseries Model

by: I Gusti Bagus A

### Import all dependencies
"""

pip install pydot==1.3.0

pip install graphviz==0.10.1

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import matplotlib.image as mpimg
from keras import callbacks
from keras.layers import Dense, LSTM
from keras.utils import plot_model
from sklearn.model_selection import train_test_split

"""### Import Dataset"""

df = pd.read_csv('validation_score.csv')

df.head()

df.info()

"""### Preprocessing"""

df = df[:12000]

"""#### Rename Unnamed column"""

df = df.rename(columns={'Unnamed: 0': 'series'})

"""##### Check Null Value in dataset"""

df.isnull().sum()

"""##### Data Visualization"""

dates = df['series'].values
median7_h  = df['median7_h'].values

plt.figure(figsize=(15,5))
plt.plot(dates, median7_h)
plt.title('Median of 7 days data with holiday',
          fontsize=20);

"""### Check Max Data Max Value and Minimum Value"""

maximum = df['median7_h'].max()
print(maximum)

minimum = df['median7_h'].min()
print(minimum)

"""#### Calculate Callback value"""

callback_value = 0.1 * (maximum - minimum)

"""### Data Splititng"""

train_median7h, test_median7h, train_dates, test_dates = train_test_split(median7_h, dates, test_size=0.2)

"""#### windowed dataset function"""

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[1:]))
    return ds.batch(batch_size).prefetch(1)

"""#### Convert the timeseries attributes into numpy"""

train_set = windowed_dataset(train_median7h, window_size=30, batch_size=50, shuffle_buffer=500)
test_set = windowed_dataset(test_median7h, window_size=30, batch_size=50, shuffle_buffer=500)

"""### Create Model"""

model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(30, return_sequences=True),
  tf.keras.layers.LSTM(30),
  tf.keras.layers.Dense(15, activation="relu"),
  tf.keras.layers.Dense(5, activation="relu"),
  tf.keras.layers.Dense(1),
])

optimizer = tf.keras.optimizers.SGD(lr=1.0000e-04, momentum=0.8)

model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])

"""### Create Callbacks"""

class myCallback(callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('mae')< callback_value):
      print("\nYour Mean Absolute Error < 10% from data scale!")
      self.model.stop_training = True

callbacks = myCallback()

"""### Fit Model"""

history = model.fit(train_set, epochs=50, callbacks=[callbacks], validation_data=(test_set), verbose=2)

print(model.summary())

plot_model(model, to_file='submission_2_web_traffic_timeseries.png')

"""### Plotting

#### Plot Model Loss
"""

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""#### Plot Model MAE"""

plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('MAE Model')
plt.ylabel('MAE')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

